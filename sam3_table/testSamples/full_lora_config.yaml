# SAM3 LoRA Training Configuration
# Full LoRA configuration - applies LoRA to ALL transformer components
# Use this for maximum adaptation capacity (more trainable parameters)

# Model settings
model:
  name: "facebook/sam3"
  cache_dir: null

# LoRA settings
lora:
  rank: 32                   # Reduced from 64 to avoid OOM
  alpha: 64                  # 2x rank (scaling factor)
  dropout: 0.1               # Some dropout for regularization

  # Target modules for SAM3 architecture
  # SAM3 uses different naming conventions than standard transformers:
  #
  # VISION BACKBONE (ViT-style):
  #   - "qkv"    : Fused Q/K/V projection (backbone.vision_backbone.trunk.blocks.X.attn.qkv)
  #   - "proj"   : Attention output projection (backbone.vision_backbone.trunk.blocks.X.attn.proj)
  #   - "fc1"    : MLP first layer (backbone.vision_backbone.trunk.blocks.X.mlp.fc1)
  #   - "fc2"    : MLP second layer (backbone.vision_backbone.trunk.blocks.X.mlp.fc2)
  #
  # LANGUAGE BACKBONE (CLIP-style):
  #   - "q_proj", "k_proj", "v_proj" : Attention projections (via MultiheadAttentionLoRA replacement)
  #   - "out_proj" : Attention output projection
  #   - "c_fc"   : MLP first layer (backbone.language_backbone.encoder.transformer.resblocks.X.mlp.c_fc)
  #   - "c_proj" : MLP second layer (backbone.language_backbone.encoder.transformer.resblocks.X.mlp.c_proj)
  #
  # TRANSFORMER ENCODER/DECODER:
  #   - "q_proj", "k_proj", "v_proj" : Attention projections (via MultiheadAttentionLoRA replacement)
  #   - "out_proj" : Attention output projection
  #   - "linear1" : FFN first layer (transformer.encoder/decoder.layers.X.linear1)
  #   - "linear2" : FFN second layer (transformer.encoder/decoder.layers.X.linear2)
  #
  target_modules:
    # Standard attention projections (for replaced MultiheadAttention modules)
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "out_proj"
    # Vision backbone (ViT-style)
    - "qkv"                  # Fused Q/K/V projection in vision backbone
    - "proj"                 # Output projection in vision backbone attention
    - "fc1"                  # Vision backbone MLP first layer
    - "fc2"                  # Vision backbone MLP second layer
    # Language backbone (CLIP-style)
    - "c_fc"                 # Language backbone MLP first layer
    - "c_proj"               # Language backbone MLP second layer
    # Transformer encoder/decoder FFN
    - "linear1"              # Transformer FFN first layer
    - "linear2"              # Transformer FFN second layer

  # Apply LoRA to ALL components for maximum adaptation
  apply_to_vision_encoder: true    # Vision backbone (ViT)
  apply_to_text_encoder: true      # Language backbone (CLIP text encoder)
  apply_to_geometry_encoder: true  # Geometry encoder for prompts
  apply_to_detr_encoder: true      # DETR-style transformer encoder
  apply_to_detr_decoder: true      # DETR-style transformer decoder
  apply_to_mask_decoder: true      # Mask prediction head

# Training settings
training:
  data:
    train:
      image_dir: "/workspace/data/train"
      annotation_file: "/workspace/data/train/_annotations.coco.json"
    valid:
      image_dir: "/workspace/data/valid"
      annotation_file: "/workspace/data/valid/_annotations.coco.json"
  batch_size: 4      # Increased from 1 to reduce gradient variance
  num_workers: 2        # Increase for better data loading

  learning_rate: 5e-5        # Increased from 1e-5 (SAM3 uses 1e-4 to 5e-4 for fine-tuning)
  weight_decay: 0.01
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_epsilon: 1e-8
  max_grad_norm: 1.0

  num_epochs: 100       # Reduced from 500 (small dataset overfits quickly)
  warmup_steps: 200     # Reduced from 1000 (proportional to dataset size)
  lr_scheduler: "cosine"

  logging_steps: 10
  eval_steps: 100       # More frequent validation (from 500)
  save_steps: 100       # More frequent checkpoints (from 1000)
  save_total_limit: 5   # Keep more checkpoints

  mixed_precision: "bf16"    # Use bfloat16 if available
  seed: 42
  gradient_accumulation_steps: 8  # Reduced from 16 (effective batch: 2Ã—8=16)

output:
  output_dir: "outputs/sam3_lora_full"
  logging_dir: "logs"
  save_lora_only: true
  push_to_hub: false
  hub_model_id: null

evaluation:
  metric: "iou"
  save_predictions: false
  compute_metrics_during_training: true

hardware:
  device: "cuda"
  dataloader_pin_memory: true
  use_compile: false
